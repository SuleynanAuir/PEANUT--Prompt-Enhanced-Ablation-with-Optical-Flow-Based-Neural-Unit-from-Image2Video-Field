# PEANUT
---

##### 1. 引言 (Introduction)

* 1.1 研究背景与动机：$2D \rightarrow 3D$ 几何感知编辑
* 1.2 挑战分析：$\text{Prompt}$ 理解、长时追踪、背景重构、$\text{STC}$
* 1.3 核心贡献：$\text{PEANUT}$ 三阶段创新框架

##### 2. 相关工作 (Related Work)

* 2.1 视频目标分割与追踪 ($\text{VOS}$ \& $\text{VOT}$)
* 2.2 视频修补与目标消除 ($\text{Inpainting}$ \& $\text{OR}$)
* 2.3 视频超分辨率 ($\text{VSR}$)

##### 3. $\text{PEANUT}$ 方法论 (Methodology)

* 3.1 框架总览：三阶段级联管线
* 3.2 **P-MASk**：**Prompt-Guided Mask Generation**
    * 3.2.1 基础架构：冻结 $\text{SAMv}2$
    * 3.2.2 跨模态时序适配器 ($\text{CMT}$)
    * 3.2.3 条件记忆编码器 ($\text{CME}$)
* 3.3 **NOF-Eraser**：**Neural Optical Flow-Based Inpainting**
    * 3.3.1 光流预测与补全 ($\text{SpyNet}$ & Flow Module)
    * 3.3.2 特征传播与几何感知 ($\text{DCN}$ & Propagation)
    * 3.3.3 时域焦点 $\text{Transformer}$ ($\text{TFT}$)
    * 3.3.4 损失函数（含 $\text{STC}$ 约束）
* 3.4 **UR-Net**：**Ultra-Resolution \& Coherence**
    * 3.4.1 $\text{BasicVSR++}$ 结构增强
    * 3.4.2 残差学习与时空特征提取

##### 4. 实验设计与结果 (Experiments)

* 4.1 数据集与评估指标 ($\text{Youtube-VOS}$ / $\text{DAVIS}$)
    * **新颖测试集**：$\text{Language}$ / $\text{Temporal}$ / $\text{Background}$ / $\text{Subject}$ Ability
* 4.2 定性结果分析：$\text{Prompt}$ 理解与复杂背景重构
* 4.3 定量比较：与 $\text{E2FGVI}$、$\text{BasicVSR++}$ 等基线对比
* 4.4 消融实验 ($\text{Ablation Study}$)：$\text{CMT}$、$\text{CME}$、$\text{TFT}$、$\text{DCN}$ 贡献验证

##### 5. 结论与未来工作 (Conclusion)
---


## 摘要与引言 (Abstract & Introduction)

### 1.1 研究背景与动机：从 $2D$ 到 $3D$ 几何感知编辑的需求。

* **当前挑战：** 现有视频编辑技术多停留在 $2D$ 像素层面，难以处理复杂的遮挡、透视变化和新视角合成，缺乏对场景**深度（$3D$ 几何）**的内在感知。
* **需求驱动：** 随着视频内容生成 (Video Generation) 需求的高涨，业界迫切需要一种**精确、语义可控**的编辑框架，能够将 $2D$ 视频流视为 $3D$ 场景的投影，从而实现**几何级别的目标消除**与**背景重建**。
* **研究目标：** 本研究旨在构建一个**提示词引导**的 $2D$ 视频编辑管线，通过深度学习推断和利用隐式的 $3D$ 几何信息（如光流），达成**高精度、高连贯性**的目标消隐任务。

### 1.2 挑战分析：提示词理解、长时序追踪、背景修复细节、时空一致性。

* **挑战 I：复杂语义理解与精确追踪 (Language \& Tracking)：** 如何有效融合**语言提示词**与**视觉时序特征**，实现对长视频中目标实体的精确、鲁棒的追踪与分割，并生成高质量的掩码序列。
* **挑战 II：深度感知与背景重构 (Background Reconstruction)：** 目标消除后，如何利用**光流**精确引导**几何消融**，避免 $2D$ 帧间修补时产生的模糊和扭曲，并重建出具有真实 $3D$ 细节的裸露背景区域。
* **挑战 III：最终结果的时空一致性 (Spatio-Temporal Consistency)：** 修补后的视频在像素和特征层面存在不连续性，如何通过超分辨率网络进行**高频细节重构**的同时，确保**时间和空间**上的平滑与连贯性。

### 1.3 核心贡献 ($\text{PEANUT}$ 框架)：明确提出三阶段管线及各项创新点。

* **框架提出：** 提出**$\text{PEANUT}$** (Prompt-Guided Eradication and Ablation with Neural Flow for Ultra-Clarity Temporal Consistency) 三阶段视频编辑框架，实现了**提示词引导的目标消隐**与**超连贯重构**。
* **创新 I：P-MASk ($\text{Prompt}$-Mask)：** 引入**跨模态时序适配器 ($\text{CMT}$) **和**条件记忆编码器 ($\text{CME}$)**，显著增强了冻结 $\text{SAMv}2$ 对长视频的**语言理解**和**鲁棒追踪**能力。
* **创新 II：NOF-Eraser ($\text{Neural Optical Flow}$)：** 设计了**时域焦点 $\text{Transformer}$ ($\text{TFT}$) **和** $\text{DCN}$ 特征传播**机制，通过**神经光流**精确指导**几何消融**，实现了复杂背景区域的高质量修补。
* **创新 III：UR-Net ($\text{Ultra-Resolution}$)：** 在 $\text{BasicVSR++}$ 基础上增强，专注于**消除修补伪影**，确保最终输出具备极致清晰度 (**Ultra-Clarity**) 和时空连贯性 (**Temporal Consistency**)。

### 1.4 论文结构。

* 本文的其余部分组织如下：第 $2$ 节回顾了相关工作；第 $3$ 节详细介绍了 $\text{PEANUT}$ 框架的三个核心阶段；第 $4$ 节展示了详尽的实验结果和消融研究；最后，第 $5$ 节对本文进行总结并展望未来工作。

---

## 2. 相关工作 (Related Work)

* **视频目标分割与追踪 ($\text{VOS}$ \& $\text{VOT}$)：**
    * 回顾基于**语义分割**和**参考图像/提示词**的 $\text{VOS}$ 发展历程。
    * 特别关注 $\text{SAM}$（Segment Anything Model）的**泛化性**和**提示词能力**，以及其在 $\text{RefVOS}$ 任务中的应用和挑战（例如**时序连贯性**和**长时记忆**）。
* **视频修补与消除 (Video Inpainting \& Object Removal)：**
    * 回顾传统的**基于传播**的修补方法。
    * 重点介绍**基于光流** ($\text{Optical Flow}$) 的端到端修补网络，如 $\text{E2FGVI}$ 的核心思想——利用光流进行特征和像素的**时序对齐**，从而实现帧间信息互补以填补缺失区域。
    * 强调视频修补中**几何一致性**和**背景细节重构**的重要性。
* **视频超分辨率 ($\text{VSR}$)：**
    * 回顾 $\text{VSR}$ 中**特征对齐**、**传播**和**重建**的核心范式。
    * 阐述 $\text{BasicVSR++}$ 等领先模型如何利用**长距离时序信息**和**残差结构**来提升清晰度和时序连贯性，并指出其在**处理修补后伪影**方面的局限性。

---

## 3. PEANUT 方法论 (The PEANUT Methodology)

### 3.1 框架总览 (Overview of the PEANUT Pipeline)

* $\text{PEANUT}$ 框架是一个**端到端**的三阶段级联管线：
    1.  **P-MASk：** 基于**提示词**生成目标掩码和追踪轨迹。
    2.  **NOF-Eraser：** 利用**神经光流**和掩码对目标进行精确**几何消融**与**背景修补**。
    3.  **UR-Net：** 对修补后的视频进行**超分辨率重构**，确保最终的**时空一致性**。
* 流程图（略，参考框架图）清晰展示了数据流和各模块的连接方式。

### 3.2 阶段 I: P-MASk (Prompt-Guided Mask Generation)

#### 3.2.1 基础架构与冻结 $\text{SAMv}2$ ($\text{P}$ 模块)。

* 采用**冻结**的 $\text{SAMv}2$ 作为**强大的分割基石**，利用其强大的语义理解和泛化能力进行**单帧初始化和提示词交互**。
* $\text{P}$ 模块将**语言提示词**、**首帧/关键帧特征**和**当前帧图像**作为输入。

#### 3.2.2 跨模态时序适配器 ($\text{CMT}$)：跨模态融合与时序感知注意机制。

* 设计 $\text{CMT}$ 以**高效融合**来自**语言模型**和**视觉 $\text{SAM}$ 编码器**的特征。
* 核心在于引入**时序感知注意机制**，计算当前帧特征与**历史帧特征**之间的相似性，以校正和稳定 $\text{SAMv}2$ 在长时序追踪中的漂移问题。

#### 3.2.3 条件记忆编码器 ($\text{CME}$)：记忆感知追踪与动态修正机制。

* $\text{CME}$ 维护一个**可学习的记忆库**，存储目标在不同外观、姿态下的**记忆感知特征**。
* 采用**记忆-自由动态修正机制**，在每一帧中，利用当前帧的预测结果**有条件地更新**记忆库，以处理目标外观的突变，实现鲁棒的**目标追踪**。

### 3.3 阶段 II: NOF-Eraser (Neural Optical Flow-Based Inpainting)

#### 3.3.1 光流预测与补全 ($\text{SpyNet}$ & Flow Completion Module)。

* 利用 **$\text{SpyNet}$** 或类似的轻量级网络估计**前后向光流** ($\mathbf{F}_{t \rightarrow t+1}, \mathbf{F}_{t \rightarrow t-1}$)。
* **光流补全模块 (Flow Completion Module)** 负责填补目标区域内的无效光流（被目标遮挡的区域），通过**邻域信息传播**推断出目标消失后背景的**真实运动趋势**，为几何消融做准备。

#### 3.3.2 特征传播与几何感知 ($\text{DCN}$ & 前后向传播)。

* 使用**可变形卷积网络 ($\text{DCN}$)** 替换标准卷积，以**自适应**地对齐相邻帧中具有**不规则运动**的特征。
* 实现**前后向特征传播**，将**相邻帧的有效特征**和**补全后的光流**融合，将来自 $3D$ 场景的**隐式几何信息**（运动/深度变化）注入到当前帧的修补中。

#### 3.3.3 时域焦点 $\text{Transformer}$ ($\text{TFT}$)：局部/非局部特征融合与软切分 ($\text{SS}$) 策略。

* $\text{TFT}$ 作为核心修补块，用于融合**局部帧特征**与**非局部传播特征**。
* 采用**软切分 ($\text{SS}$, Soft-Split)** 策略进行 $\text{Patch}$ $\text{Embedding}$，**保留 patch 边界信息**，增强特征连续性。
* 利用**局部窗口注意机制**捕捉短时序细节，并结合**全局注意机制**确保长时序的**语义连贯性**。

#### 3.3.4 损失函数：修补损失、对抗损失、时空平滑损失。

* **修补损失：** 采用 $\mathcal{L}_1$ / $\mathcal{L}_2$ 和感知损失 ($\mathcal{L}_{\text{perceptual}}$) 确保像素和语义的准确性。
* **对抗损失：** 引入 $\mathcal{L}_{\text{GAN}}$ 提高修补区域的**真实感**。
* **时空平滑损失：** 增加基于**光流**的损失项，惩罚相邻帧修补结果中的**不一致性**，强制保持**时空连贯性**。

### 3.4 阶段 III: UR-Net (Ultra-Resolution \& Coherence)

#### 3.4.1 基于 $\text{BasicVSR++}$ 的结构增强。

* 采用 $\text{BasicVSR++}$ 的**高鲁棒性结构**作为 $\text{UR-Net}$ 的基础，利用其强大的**长距离特征传播能力**。

#### 3.4.2 残差学习与时空特征提取。

* 采用**残差学习**策略，专注于从修补后的视频中恢复**高频纹理细节**和**抵消修补引入的伪影**。
* 优化时空特征提取模块，增强对**修补区域边界**和**重建背景区域**的细节捕获能力。

#### 3.4.3 输出：Spatio-Temporal Consistency。

* 最终 $\text{UR-Net}$ 的输出不仅是高分辨率视频，更是经过**时空连贯性**（$\text{STC}$）约束和优化的结果，确保编辑和重构结果在时间轴上**平滑且逼真**。

---

## 4. 实验设计与结果 (Experiments and Results)

### 4.1 数据集与评估指标

* **训练集：** 采用 $\text{Youtube-VOS}$ 和 $\text{DAVIS}$ 等大规模视频分割与追踪数据集，以提供丰富的目标实例和时序数据。
* **新颖测试集：** 构建包含四个核心能力的定制化测试子集：
    * **I. Language Ability：** 包含复杂、模糊的**提示词**，测试 $\text{P-MASk}$ 的语义理解能力。
    * **II. Temporal Ability：** 包含长时程、快速运动和临时遮挡的视频，测试**时序追踪**和**长时序修补**的连贯性。
    * **III. Background Reconstruction Ability：** 包含复杂纹理和 $3D$ 结构的背景（如砖墙、植被），测试 $\text{NOF-Eraser}$ 的**背景修复细节**。
    * **IV. Foreground/Background and Subject Ability：** 包含多个相似对象或目标与背景高度相似的场景，测试模型的**主体区分度**。
* **指标：**
    * **清晰度与保真度：** $\text{PSNR}$ 和 $\text{SSIM}$。
    * **感知质量：** $\text{LPIPS}$ (感知损失)，用于衡量图像的**真实感和视觉质量**。
    * **时序一致性：** **Warp Error** 或 $\text{Temporal Consistency Score}$ (基于光流对齐的前后帧差异)，衡量**时空连贯性**。

### 4.2 定性结果分析

* 展示 $\text{PEANUT}$ 在**复杂语言提示词**下的**精准掩码生成**案例。
* 对比**修补细节**：突出 $\text{NOF-Eraser}$ 在**复杂背景重构**和**几何边界平滑**上的优势，证明其优于 $2D$ Inpainting 方法。
* 展示最终输出视频在**高分辨率下**的**时序平滑性**。

### 4.3 定量比较

* 与基线方法 ($\text{E2FGVI}$、$\text{BasicVSR++}$ 单独运行、以及其他 $\text{RefVOS}$ + Inpainting 组合) 在所有**评估指标**和**新颖测试子集**上的性能对比。
* 突出 $\text{PEANUT}$ 在**Temporal Consistency**和**LPIPS**指标上的领先地位。

### 4.4 消融实验 (Ablation Study)

* **P-MASk 模块：** 验证** $\text{CMT}$ **和** $\text{CME}$** 对**追踪准确率**和**语言对齐度**的贡献。
* **NOF-Eraser 模块：** 验证**光流补全模块**、** $\text{DCN}$ 特征传播**以及** $\text{TFT}$ **结构对**修补质量**的提升作用。
* **UR-Net 模块：** 验证 $\text{UR-Net}$ 的增强结构对**时空一致性**和**伪影消除**的关键性。

---

## 5. 结论与未来工作 (Conclusion \& Future Work)

* **结论：** 总结 $\text{PEANUT}$ 框架通过结合**语言引导**、**神经光流消融**和**超连贯重构**，成功解决了 $2D \rightarrow 3D$ 视频编辑中的**时序、语义和几何**挑战，实现了当前最先进的性能。
* **未来工作：**
    * 探索将 $2D$ 流程中的**隐式 $3D$ 几何**（如深度图）**显式化**，进一步提升几何感知能力。
    * 研究**实时化**的 $\text{PEANUT}$ 框架，以应用于更广泛的工业场景。
    * 拓展至**多目标、多提示词**的复杂交互编辑任务。